{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a946072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import os\n",
    "import numpy as npP\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c44a1160",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "from prophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "1d13b656",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_columns = [\n",
    "    'resting_heart_rate',\n",
    "    'avg_overall_sleep_score',\n",
    "    'fatigue',\n",
    "    'mood',\n",
    "    'stress',\n",
    "    'sleep_quality',\n",
    "    'very_active_minutes_sum',\n",
    "    'sedentary_minutes_sum',\n",
    "    'resting_heart_rate_14_mean',\n",
    "    'resting_heart_rate_14_std',\n",
    "    'avg_overall_sleep_score_14_mean',\n",
    "    'avg_overall_sleep_score_14_std',\n",
    "]\n",
    "\n",
    "y_column = 'Is_High_Risk_Next_7_Days'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "10a79ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resting_heart_rate</th>\n",
       "      <th>avg_overall_sleep_score</th>\n",
       "      <th>fatigue</th>\n",
       "      <th>mood</th>\n",
       "      <th>stress</th>\n",
       "      <th>sleep_quality</th>\n",
       "      <th>very_active_minutes_sum</th>\n",
       "      <th>sedentary_minutes_sum</th>\n",
       "      <th>resting_heart_rate_14_mean</th>\n",
       "      <th>resting_heart_rate_14_std</th>\n",
       "      <th>avg_overall_sleep_score_14_mean</th>\n",
       "      <th>avg_overall_sleep_score_14_std</th>\n",
       "      <th>Subjective_Distress_Score</th>\n",
       "      <th>Physiological_Deviation_Score</th>\n",
       "      <th>Composite_Risk_Score</th>\n",
       "      <th>High_Risk_State</th>\n",
       "      <th>Is_High_Risk_Next_7_Days</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-11-21</th>\n",
       "      <td>55.481817</td>\n",
       "      <td>84.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>94</td>\n",
       "      <td>517</td>\n",
       "      <td>54.523219</td>\n",
       "      <td>1.594162</td>\n",
       "      <td>78.428571</td>\n",
       "      <td>7.977992</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-22</th>\n",
       "      <td>57.737835</td>\n",
       "      <td>67.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9</td>\n",
       "      <td>830</td>\n",
       "      <td>54.804446</td>\n",
       "      <td>1.791905</td>\n",
       "      <td>77.357143</td>\n",
       "      <td>8.454468</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-23</th>\n",
       "      <td>56.930720</td>\n",
       "      <td>85.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>30</td>\n",
       "      <td>623</td>\n",
       "      <td>55.039024</td>\n",
       "      <td>1.842910</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>8.682431</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-24</th>\n",
       "      <td>59.165880</td>\n",
       "      <td>48.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>86</td>\n",
       "      <td>855</td>\n",
       "      <td>55.423071</td>\n",
       "      <td>2.104138</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>11.832160</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-25</th>\n",
       "      <td>56.551208</td>\n",
       "      <td>89.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>49</td>\n",
       "      <td>537</td>\n",
       "      <td>55.704120</td>\n",
       "      <td>1.958144</td>\n",
       "      <td>76.214286</td>\n",
       "      <td>12.052313</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            resting_heart_rate  avg_overall_sleep_score  fatigue  mood  \\\n",
       "dateTime                                                                 \n",
       "2019-11-21           55.481817                     84.0      2.0   3.0   \n",
       "2019-11-22           57.737835                     67.0      3.0   3.0   \n",
       "2019-11-23           56.930720                     85.0      3.0   3.0   \n",
       "2019-11-24           59.165880                     48.0      3.0   3.0   \n",
       "2019-11-25           56.551208                     89.0      3.0   3.0   \n",
       "\n",
       "            stress  sleep_quality  very_active_minutes_sum  \\\n",
       "dateTime                                                     \n",
       "2019-11-21     4.0            3.0                       94   \n",
       "2019-11-22     3.0            2.0                        9   \n",
       "2019-11-23     4.0            4.0                       30   \n",
       "2019-11-24     4.0            3.0                       86   \n",
       "2019-11-25     4.0            4.0                       49   \n",
       "\n",
       "            sedentary_minutes_sum  resting_heart_rate_14_mean  \\\n",
       "dateTime                                                        \n",
       "2019-11-21                    517                   54.523219   \n",
       "2019-11-22                    830                   54.804446   \n",
       "2019-11-23                    623                   55.039024   \n",
       "2019-11-24                    855                   55.423071   \n",
       "2019-11-25                    537                   55.704120   \n",
       "\n",
       "            resting_heart_rate_14_std  avg_overall_sleep_score_14_mean  \\\n",
       "dateTime                                                                 \n",
       "2019-11-21                   1.594162                        78.428571   \n",
       "2019-11-22                   1.791905                        77.357143   \n",
       "2019-11-23                   1.842910                        78.000000   \n",
       "2019-11-24                   2.104138                        76.000000   \n",
       "2019-11-25                   1.958144                        76.214286   \n",
       "\n",
       "            avg_overall_sleep_score_14_std  Subjective_Distress_Score  \\\n",
       "dateTime                                                                \n",
       "2019-11-21                        7.977992                          1   \n",
       "2019-11-22                        8.454468                          1   \n",
       "2019-11-23                        8.682431                          0   \n",
       "2019-11-24                       11.832160                          0   \n",
       "2019-11-25                       12.052313                          0   \n",
       "\n",
       "            Physiological_Deviation_Score  Composite_Risk_Score  \\\n",
       "dateTime                                                          \n",
       "2019-11-21                              0                     1   \n",
       "2019-11-22                              2                     3   \n",
       "2019-11-23                              1                     1   \n",
       "2019-11-24                              2                     2   \n",
       "2019-11-25                              0                     0   \n",
       "\n",
       "            High_Risk_State  Is_High_Risk_Next_7_Days  \n",
       "dateTime                                               \n",
       "2019-11-21                0                         0  \n",
       "2019-11-22                1                         0  \n",
       "2019-11-23                0                         1  \n",
       "2019-11-24                0                         1  \n",
       "2019-11-25                0                         1  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./cleaned_data/p07_model_ready.csv\")\n",
    "df.set_index(\"dateTime\", inplace=True)\n",
    "df.sort_index(inplace=True)\n",
    "assert df.isna().sum().sum() == 0, \"Data contains NaN values\"\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4108bdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[X_columns]\n",
    "y = df[y_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "12252d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, test_size=0.2):\n",
    "    # Split the data into training and testing sets\n",
    "    split_index = int(len(X) * (1 - test_size))\n",
    "    X_train, X_test = X[:split_index], X[split_index:]\n",
    "    y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "    X_train.index = pd.to_datetime(X_train.index, errors='coerce')\n",
    "    X_test.index = pd.to_datetime(X_test.index, errors='coerce')    \n",
    "\n",
    "    y_train.index = pd.to_datetime(y_train.index, errors='coerce')\n",
    "    y_test.index = pd.to_datetime(y_test.index, errors='coerce')\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e275868e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_arima_model(X, y, order=(1, 0, 0)):\n",
    "    model = ARIMA(y, exog=X, order=order)\n",
    "    model_fit = model.fit()\n",
    "    return model_fit\n",
    "\n",
    "def make_predictions(model_fit, X):\n",
    "    predictions = model_fit.predict(start=len(X) - len(X), end=len(X) - 1, exogenous=X)\n",
    "    return predictions\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2_score_value = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)   \n",
    "    return mse, r2_score_value, mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b00360",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bf0b8b",
   "metadata": {},
   "source": [
    "models = [\n",
    "    {\n",
    "        \"name\": \"SARIMAX\",\n",
    "        \"call\": SARIMAX,\n",
    "        \"params\": {\n",
    "            \"order\": (1, 1, 1),\n",
    "            \"seasonal_order\": (1, 1, 1, 24)\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"LinearRegression\",\n",
    "        \"call\": LinearRegression,\n",
    "        \"params\": {}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"XGBoost\",\n",
    "        \"call\": XGBRegressor,\n",
    "        \"params\": {\n",
    "            \"n_estimators\": 100,\n",
    "            \"max_depth\": 3,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"objective\": \"reg:squarederror\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"LightGBM\",\n",
    "        \"call\": LGBMRegressor,\n",
    "        \"params\": {\n",
    "            \"n_estimators\": 100,\n",
    "            \"max_depth\": 3,\n",
    "            \"learning_rate\": 0.1\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"RandomForest\",\n",
    "        \"call\": RandomForestRegressor,\n",
    "        \"params\": {\n",
    "            \"n_estimators\": 100,\n",
    "            \"max_depth\": 5\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"GradientBoosting\",\n",
    "        \"call\": GradientBoostingRegressor,\n",
    "        \"params\": {\n",
    "            \"n_estimators\": 100,\n",
    "            \"max_depth\": 3,\n",
    "            \"learning_rate\": 0.1\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ARIMA\",\n",
    "        \"call\": ARIMA,\n",
    "        \"params\": {\n",
    "            \"order\": (1, 1, 1)\n",
    "        }\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "90ee0b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    {\n",
    "        \"name\": \"LogisticRegression\",\n",
    "        \"call\": LogisticRegression,\n",
    "        \"params\": {\n",
    "            \"max_iter\": 1000,\n",
    "            \"solver\": \"liblinear\",\n",
    "            \"C\": 1.0,\n",
    "            \"class_weight\": \"balanced\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"XGBoost\",\n",
    "        \"call\": XGBClassifier,      \n",
    "        \"params\": {\n",
    "            \"n_estimators\": 100,\n",
    "            \"max_depth\": 3,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"objective\": \"binary:logistic\",\n",
    "            \"scale_pos_weight\": 1,\n",
    "            \"subsample\": 0.8,\n",
    "            \"colsample_bytree\": 0.8\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"LightGBM\",\n",
    "        \"call\": LGBMClassifier,\n",
    "        \"params\": {\n",
    "            \"n_estimators\": 100,\n",
    "            \"max_depth\": 3,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"objective\": \"binary\",\n",
    "            \"class_weight\": \"balanced\",\n",
    "            \"boosting_type\": \"gbdt\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"RandomForest\",\n",
    "        \"call\": RandomForestClassifier,\n",
    "        \"params\": {\n",
    "            \"n_estimators\": 100,\n",
    "            \"max_depth\": 5,\n",
    "            \"class_weight\": \"balanced\",\n",
    "            \"min_samples_split\": 5,\n",
    "            \"min_samples_leaf\": 2\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"GradientBoosting\",\n",
    "        \"call\": GradientBoostingClassifier,\n",
    "        \"params\": {\n",
    "            \"n_estimators\": 100,\n",
    "            \"max_depth\": 3,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"subsample\": 0.8,\n",
    "            \"min_samples_split\": 5\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"SVC\",\n",
    "        \"call\": SVC,\n",
    "        \"params\": {\n",
    "            \"kernel\": 'linear',\n",
    "            \"C\": 1.0,\n",
    "            \"class_weight\": \"balanced\",\n",
    "            \"probability\": True\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"DecisionTree\",\n",
    "        \"call\": DecisionTreeClassifier,\n",
    "        \"params\": {\n",
    "            \"max_depth\": 5,\n",
    "            \"class_weight\": \"balanced\",\n",
    "            \"min_samples_split\": 5,\n",
    "            \"min_samples_leaf\": 2\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"KNeighbors\",\n",
    "        \"call\": KNeighborsClassifier,\n",
    "        \"params\": {\n",
    "            \"n_neighbors\": 5,\n",
    "            \"weights\": \"distance\",\n",
    "            \"metric\": \"minkowski\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"NaiveBayes\",\n",
    "        \"call\": GaussianNB,\n",
    "        \"params\": {\n",
    "            \"var_smoothing\": 1e-9\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "time_series_models = [\n",
    "    {\n",
    "        \"name\": \"SARIMAX\",\n",
    "        \"call\": SARIMAX,\n",
    "        \"params\": {\n",
    "            \"order\": (1, 1, 1),\n",
    "            \"seasonal_order\": (1, 1, 1, 7),  # Weekly seasonality\n",
    "            \"enforce_stationarity\": False,\n",
    "            \"enforce_invertibility\": False\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"SARIMAX_Daily\",\n",
    "        \"call\": SARIMAX,\n",
    "        \"params\": {\n",
    "            \"order\": (2, 1, 2),\n",
    "            \"seasonal_order\": (1, 1, 1, 1),  # Daily seasonality\n",
    "            \"enforce_stationarity\": False,\n",
    "            \"enforce_invertibility\": False\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ARIMA_Simple\",\n",
    "        \"call\": ARIMA,\n",
    "        \"params\": {\n",
    "            \"order\": (1, 1, 1),\n",
    "            \"trend\": 'c'\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ARIMA_Complex\",\n",
    "        \"call\": ARIMA,\n",
    "        \"params\": {\n",
    "            \"order\": (2, 1, 2),\n",
    "            \"trend\": 'c'\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"name\": \"Prophet\",\n",
    "        \"call\": Prophet,\n",
    "        \"params\": {\n",
    "            \"yearly_seasonality\": True,\n",
    "            \"weekly_seasonality\": True,\n",
    "            \"daily_seasonality\": True,\n",
    "            \"changepoint_prior_scale\": 0.05\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"VAR\",\n",
    "        \"call\": VAR,\n",
    "        \"params\": {\n",
    "            \"maxlags\": 15,\n",
    "            \"ic\": 'aic'\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"SARIMA_Weekly\",\n",
    "        \"call\": SARIMAX,\n",
    "        \"params\": {\n",
    "            \"order\": (1, 0, 1),\n",
    "            \"seasonal_order\": (1, 1, 1, 7),\n",
    "            \"trend\": 'c'\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"SARIMA_Monthly\",\n",
    "        \"call\": SARIMAX,\n",
    "        \"params\": {\n",
    "            \"order\": (1, 0, 1),\n",
    "            \"seasonal_order\": (1, 1, 1, 30),\n",
    "            \"trend\": 'c'\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "b501eff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(models, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Trains multiple time series models on the provided training data.\n",
    "    Args:\n",
    "        models (list): List of dictionaries containing model names, calls, and parameters.\n",
    "        X_train (pd.DataFrame or np.ndarray): Training features (exogenous variables).\n",
    "        y_train (pd.Series or np.ndarray): Training target series.\n",
    "    Returns:\n",
    "        dict: Dictionary containing trained models.\n",
    "    \"\"\"\n",
    "\n",
    "    trained_models = {}\n",
    "    if not isinstance(models, list):\n",
    "        raise ValueError(\"The 'models' argument must be a list of dictionaries containing model information.\")\n",
    "    if len(y_train) == 0:\n",
    "        raise ValueError(\"The training data is empty. Please provide valid data for training.\")\n",
    "\n",
    "    for model in models:\n",
    "        model_name = model[\"name\"]\n",
    "        model_call = model[\"call\"]\n",
    "        model_params = model[\"params\"]\n",
    "        print(f\"Preparing to train {model_name} with parameters: {model_params}\")\n",
    "        # Ensure the models directory exists\n",
    "        if not os.path.exists(\"./models\"):\n",
    "            os.makedirs(\"./models\")\n",
    "        if model_name in [\"SARIMAX\", \"ARIMA\"]:\n",
    "            # Pass exogenous variables if available\n",
    "            if X_train is not None and X_train.shape[1] > 0:\n",
    "                trained_model = model_call(y_train, exog=X_train, **model_params).fit()\n",
    "            else:\n",
    "                trained_model = model_call(y_train, **model_params).fit()\n",
    "            # joblib.dump(trained_model, f\"./models/{model_name}_model.pkl\")\n",
    "        else:\n",
    "            trained_model = model_call(**model_params).fit(X_train, y_train)\n",
    "            # joblib.dump(trained_model, f\"./models/{model_name}_model.pkl\")\n",
    "        \n",
    "        trained_models[model_name] = trained_model\n",
    "        print(f\"{model_name} trained successfully.\\n\")\n",
    "    print(\"All models trained successfully.\")\n",
    "\n",
    "    return trained_models\n",
    "\n",
    "def train_ml_models(models, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Trains multiple models on the provided training data.\n",
    "    Args:\n",
    "        models (list): List of dictionaries containing model names, calls, and parameters.\n",
    "        X_train (pd.DataFrame or np.ndarray): Training features (exogenous variables).\n",
    "        y_train (pd.Series or np.ndarray): Training target series.\n",
    "    Returns:\n",
    "        dict: Dictionary containing trained non-time series models.\n",
    "    \"\"\"\n",
    "    \n",
    "    trained_models = {}\n",
    "    if not isinstance(models, list):\n",
    "        raise ValueError(\"The 'models' argument must be a list of dictionaries containing model information.\")\n",
    "    if len(y_train) == 0:\n",
    "        raise ValueError(\"The training data is empty. Please provide valid data for training.\")\n",
    "\n",
    "    for model in models:\n",
    "        model_name = model[\"name\"]\n",
    "        model_call = model[\"call\"]\n",
    "        model_params = model[\"params\"]\n",
    "        print(f\"Preparing to train {model_name} with parameters: {model_params}\")\n",
    "        \n",
    "        if not os.path.exists(\"./models\"):\n",
    "            os.makedirs(\"./models\")\n",
    "        \n",
    "        trained_model = model_call(**model_params).fit(X_train, y_train)\n",
    "        #joblib.dump(trained_model, f\"./models/{model_name}_model.pkl\")\n",
    "        \n",
    "        trained_models[model_name] = trained_model\n",
    "        print(f\"{model_name} trained successfully.\\n\")\n",
    "    \n",
    "    print(\"All models trained successfully.\")\n",
    "    \n",
    "    return trained_models\n",
    "\n",
    "def train_time_series_models(models, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Trains multiple time series models on the provided training data.\n",
    "    \"\"\"\n",
    "    trained_time_series_models = {}\n",
    "    \n",
    "    for model in models:\n",
    "        model_name = model[\"name\"]\n",
    "        model_call = model[\"call\"]\n",
    "        model_params = model[\"params\"]\n",
    "        print(f\"Preparing to train {model_name} with parameters: {model_params}\")\n",
    "        \n",
    "        try:\n",
    "            if model_name in [\"SARIMAX\", \"ARIMA\", \"SARIMAX_Daily\", \"ARIMA_Simple\", \n",
    "                            \"ARIMA_Complex\", \"SARIMA_Weekly\", \"SARIMA_Monthly\"]:\n",
    "                # For SARIMAX/ARIMA models, pass endog as first argument\n",
    "                model_instance = model_call(endog=y_train, exog=X_train, **model_params)\n",
    "                trained_model = model_instance.fit(disp=False)\n",
    "            \n",
    "            elif model_name == \"VAR\":\n",
    "                # For VAR, combine X and y into one dataframe\n",
    "                combined_data = pd.concat([y_train, X_train], axis=1)\n",
    "                model_instance = model_call(combined_data)\n",
    "                trained_model = model_instance.fit(**model_params)\n",
    "            \n",
    "            elif model_name == \"Prophet\":\n",
    "                # For Prophet, prepare data in required format\n",
    "                prophet_data = pd.DataFrame({\n",
    "                    'ds': y_train.index,\n",
    "                    'y': y_train.values\n",
    "                })\n",
    "                model_instance = model_call(**model_params)\n",
    "                for column in X_train.columns:\n",
    "                    model_instance.add_regressor(column)\n",
    "                trained_model = model_instance.fit(prophet_data)\n",
    "            \n",
    "            else:\n",
    "                trained_model = model_call(**model_params).fit(X_train, y_train)\n",
    "            \n",
    "            trained_time_series_models[model_name] = trained_model\n",
    "            print(f\"{model_name} trained successfully.\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error training {model_name}: {str(e)}\\n\")\n",
    "            continue\n",
    "    \n",
    "    print(\"Time series models training complete.\")\n",
    "    return trained_time_series_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "72b09a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(trained_models,X_test, y_test,model_type = \"classification\"):\n",
    "    \"\"\"\n",
    "    Evaluates trained models on the test data and calculates Mean Squared Error (MSE).\n",
    "    Args:\n",
    "        trained_models (dict): Dictionary containing trained models.\n",
    "        X_test (pd.DataFrame): Test features.\n",
    "        y_test (pd.Series): Test target.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing model names, parameters, and MSE values.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for model_name, model in trained_models.items():\n",
    "        print(f\"Evaluating {model_name}...\")\n",
    "        if model_type in \"time_series\":\n",
    "            start = len(model.data.endog)\n",
    "            end = start + len(y_test) - 1\n",
    "            predictions = model.predict(start=start, end=end, exog=X_test)\n",
    "            predictions = predictions[:len(y_test)]  # Ensure predictions match test data length\n",
    "\n",
    "            if np.isnan(predictions).any():\n",
    "                print(f\"Warning: {model_name} predictions contain NaN values. Skipping evaluation for this model.\")\n",
    "                continue\n",
    "            if len(predictions) != len(y_test):\n",
    "                print(f\"Warning: {model_name} predictions length does not match test data length. Skipping evaluation for this model.\")\n",
    "                continue\n",
    "            mse = mean_squared_error(y_test, predictions)\n",
    "            r2_score_value = r2_score(y_test, predictions)\n",
    "            mae = mean_absolute_error(y_test, predictions)\n",
    "            print(f\"{model_name} MSE: {mse:.4f}, R^2: {r2_score_value:.4f}, MAE: {mae:.4f}\")\n",
    "            results.append({\n",
    "                \"Model\": model_name,\n",
    "                \"Parameters\": model.params,\n",
    "                \"MSE\": mse,\n",
    "                \"MAE\": mae,\n",
    "                \"R^2\": r2_score_value, \n",
    "            })\n",
    "        elif model_type == \"regresssion\":\n",
    "            predictions = model.predict(X_test)\n",
    "            if len(predictions) != len(y_test):\n",
    "                print(f\"Warning: {model_name} predictions length does not match test data length. Skipping evaluation for this model.\")\n",
    "                continue\n",
    "            mae = mean_absolute_error(y_test.values, predictions)\n",
    "            mse = mean_squared_error(y_test.values, predictions)\n",
    "            r2_score_value = r2_score(y_test.values, predictions)\n",
    "            print(f\"{model_name} MSE: {mse:.4f}, R^2: {r2_score_value:.4f}, MAE: {mae:.4f}\")\n",
    "            results.append({\n",
    "                \"Model\": model_name,\n",
    "                \"Parameters\": model.get_params(),\n",
    "                \"MSE\": mse,\n",
    "                \"MAE\": mae,\n",
    "                \"R^2\": r2_score_value,                \n",
    "            })\n",
    "        elif model_type == \"classification\":\n",
    "            predictions = model.predict(X_test)\n",
    "            if len(predictions) != len(y_test):\n",
    "                print(f\"Warning: {model_name} predictions length does not match test data length. Skipping evaluation for this model.\")\n",
    "                continue\n",
    "            accuracy = accuracy_score(y_test, predictions)\n",
    "            f1 = f1_score(y_test, predictions, average='weighted')\n",
    "            precision = precision_score(y_test, predictions, average='weighted')\n",
    "            recall = recall_score(y_test, predictions, average='weighted')\n",
    "            print(f\"{model_name} Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "            results.append({\n",
    "                \"Model\": model_name,\n",
    "                \"Parameters\": model.get_params(),\n",
    "                \"Accuracy\": accuracy,\n",
    "                \"F1 Score\": f1,\n",
    "                \"Precision\": precision,\n",
    "                \"Recall\": recall,\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Model {model_name} is not recognized for evaluation.\")\n",
    "    print(\"Evaluation complete.\")\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8213b0c1",
   "metadata": {},
   "source": [
    "## testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "1c312df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "assert X_train.index.dtype == 'datetime64[ns]', \"X_train index is not datetime\"\n",
    "assert X_test.index.dtype == 'datetime64[ns]', \"X_test index is not datetime\"  \n",
    "assert y_train.index.dtype == 'datetime64[ns]', \"y_train index is not datetime\"\n",
    "assert y_test.index.dtype == 'datetime64[ns]', \"y_test index is not datetime\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ba2224c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to train LogisticRegression with parameters: {'max_iter': 1000, 'solver': 'liblinear', 'C': 1.0, 'class_weight': 'balanced'}\n",
      "LogisticRegression trained successfully.\n",
      "\n",
      "Preparing to train XGBoost with parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.1, 'objective': 'binary:logistic', 'scale_pos_weight': 1, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "XGBoost trained successfully.\n",
      "\n",
      "Preparing to train LightGBM with parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.1, 'objective': 'binary', 'class_weight': 'balanced', 'boosting_type': 'gbdt'}\n",
      "[LightGBM] [Info] Number of positive: 52, number of negative: 48\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000193 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 263\n",
      "[LightGBM] [Info] Number of data points in the train set: 100, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LightGBM trained successfully.\n",
      "\n",
      "Preparing to train RandomForest with parameters: {'n_estimators': 100, 'max_depth': 5, 'class_weight': 'balanced', 'min_samples_split': 5, 'min_samples_leaf': 2}\n",
      "RandomForest trained successfully.\n",
      "\n",
      "Preparing to train GradientBoosting with parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.1, 'subsample': 0.8, 'min_samples_split': 5}\n",
      "GradientBoosting trained successfully.\n",
      "\n",
      "Preparing to train SVC with parameters: {'kernel': 'linear', 'C': 1.0, 'class_weight': 'balanced', 'probability': True}\n",
      "SVC trained successfully.\n",
      "\n",
      "Preparing to train DecisionTree with parameters: {'max_depth': 5, 'class_weight': 'balanced', 'min_samples_split': 5, 'min_samples_leaf': 2}\n",
      "DecisionTree trained successfully.\n",
      "\n",
      "Preparing to train KNeighbors with parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski'}\n",
      "KNeighbors trained successfully.\n",
      "\n",
      "Preparing to train NaiveBayes with parameters: {'var_smoothing': 1e-09}\n",
      "NaiveBayes trained successfully.\n",
      "\n",
      "All models trained successfully.\n"
     ]
    }
   ],
   "source": [
    "models = train_ml_models(classifiers, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "fd54e45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to train SARIMAX with parameters: {'order': (1, 1, 1), 'seasonal_order': (1, 1, 1, 7), 'enforce_stationarity': False, 'enforce_invertibility': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmed\\miniconda3\\envs\\ds1\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\ahmed\\miniconda3\\envs\\ds1\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\ahmed\\miniconda3\\envs\\ds1\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "c:\\Users\\ahmed\\miniconda3\\envs\\ds1\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\ahmed\\miniconda3\\envs\\ds1\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SARIMAX trained successfully.\n",
      "\n",
      "Preparing to train SARIMAX_Daily with parameters: {'order': (2, 1, 2), 'seasonal_order': (1, 1, 1, 1), 'enforce_stationarity': False, 'enforce_invertibility': False}\n",
      "Error training SARIMAX_Daily: Seasonal periodicity must be greater than 1.\n",
      "\n",
      "Preparing to train ARIMA_Simple with parameters: {'order': (1, 1, 1), 'trend': 'c'}\n",
      "Error training ARIMA_Simple: In models with integration (`d > 0`) or seasonal integration (`D > 0`), trend terms of lower order than `d + D` cannot be (as they would be eliminated due to the differencing operation). For example, a constant cannot be included in an ARIMA(1, 1, 1) model, but including a linear trend, which would have the same effect as fitting a constant to the differenced data, is allowed.\n",
      "\n",
      "Preparing to train ARIMA_Complex with parameters: {'order': (2, 1, 2), 'trend': 'c'}\n",
      "Error training ARIMA_Complex: In models with integration (`d > 0`) or seasonal integration (`D > 0`), trend terms of lower order than `d + D` cannot be (as they would be eliminated due to the differencing operation). For example, a constant cannot be included in an ARIMA(1, 1, 1) model, but including a linear trend, which would have the same effect as fitting a constant to the differenced data, is allowed.\n",
      "\n",
      "Preparing to train Prophet with parameters: {'yearly_seasonality': True, 'weekly_seasonality': True, 'daily_seasonality': True, 'changepoint_prior_scale': 0.05}\n",
      "Error training Prophet: Regressor 'resting_heart_rate' missing from dataframe\n",
      "\n",
      "Preparing to train VAR with parameters: {'maxlags': 15, 'ic': 'aic'}\n",
      "Error training VAR: maxlags is too large for the number of observations and the number of equations. The largest model cannot be estimated.\n",
      "\n",
      "Preparing to train SARIMA_Weekly with parameters: {'order': (1, 0, 1), 'seasonal_order': (1, 1, 1, 7), 'trend': 'c'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmed\\miniconda3\\envs\\ds1\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\ahmed\\miniconda3\\envs\\ds1\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\ahmed\\miniconda3\\envs\\ds1\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\ahmed\\miniconda3\\envs\\ds1\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "c:\\Users\\ahmed\\miniconda3\\envs\\ds1\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\ahmed\\miniconda3\\envs\\ds1\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\ahmed\\miniconda3\\envs\\ds1\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:866: UserWarning: Too few observations to estimate starting parameters for seasonal ARMA. All parameters except for variances will be set to zeros.\n",
      "  warn('Too few observations to estimate starting parameters%s.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SARIMA_Weekly trained successfully.\n",
      "\n",
      "Preparing to train SARIMA_Monthly with parameters: {'order': (1, 0, 1), 'seasonal_order': (1, 1, 1, 30), 'trend': 'c'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmed\\miniconda3\\envs\\ds1\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SARIMA_Monthly trained successfully.\n",
      "\n",
      "Time series models training complete.\n"
     ]
    }
   ],
   "source": [
    "time_series_models_trained = train_time_series_models(time_series_models, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "1437661b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating LogisticRegression...\n",
      "LogisticRegression Accuracy: 0.4000, F1 Score: 0.4306, Precision: 0.4897, Recall: 0.4000\n",
      "Evaluating XGBoost...\n",
      "XGBoost Accuracy: 0.6400, F1 Score: 0.6048, Precision: 0.5843, Recall: 0.6400\n",
      "Evaluating LightGBM...\n",
      "LightGBM Accuracy: 0.6400, F1 Score: 0.6310, Precision: 0.6239, Recall: 0.6400\n",
      "Evaluating RandomForest...\n",
      "RandomForest Accuracy: 0.6400, F1 Score: 0.5620, Precision: 0.5009, Recall: 0.6400\n",
      "Evaluating GradientBoosting...\n",
      "GradientBoosting Accuracy: 0.6400, F1 Score: 0.6048, Precision: 0.5843, Recall: 0.6400\n",
      "Evaluating SVC...\n",
      "SVC Accuracy: 0.4400, F1 Score: 0.4672, Precision: 0.5138, Recall: 0.4400\n",
      "Evaluating DecisionTree...\n",
      "DecisionTree Accuracy: 0.5600, F1 Score: 0.5169, Precision: 0.4800, Recall: 0.5600\n",
      "Evaluating KNeighbors...\n",
      "KNeighbors Accuracy: 0.4800, F1 Score: 0.5065, Precision: 0.5685, Recall: 0.4800\n",
      "Evaluating NaiveBayes...\n",
      "NaiveBayes Accuracy: 0.4000, F1 Score: 0.4114, Precision: 0.4235, Recall: 0.4000\n",
      "Evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = evaluate_models(models, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "b26c0353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{'C': 1.0, 'class_weight': 'balanced', 'dual':...</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.430560</td>\n",
       "      <td>0.489744</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>{'objective': 'binary:logistic', 'base_score':...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.604755</td>\n",
       "      <td>0.584286</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': 'bal...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.631019</td>\n",
       "      <td>0.623860</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>{'bootstrap': True, 'ccp_alpha': 0.0, 'class_w...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.561951</td>\n",
       "      <td>0.500870</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>{'ccp_alpha': 0.0, 'criterion': 'friedman_mse'...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.604755</td>\n",
       "      <td>0.584286</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC</td>\n",
       "      <td>{'C': 1.0, 'break_ties': False, 'cache_size': ...</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.467222</td>\n",
       "      <td>0.513766</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>{'ccp_alpha': 0.0, 'class_weight': 'balanced',...</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.516923</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KNeighbors</td>\n",
       "      <td>{'algorithm': 'auto', 'leaf_size': 30, 'metric...</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.506486</td>\n",
       "      <td>0.568462</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaiveBayes</td>\n",
       "      <td>{'priors': None, 'var_smoothing': 1e-09}</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.411429</td>\n",
       "      <td>0.423529</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Model                                         Parameters  \\\n",
       "0  LogisticRegression  {'C': 1.0, 'class_weight': 'balanced', 'dual':...   \n",
       "1             XGBoost  {'objective': 'binary:logistic', 'base_score':...   \n",
       "2            LightGBM  {'boosting_type': 'gbdt', 'class_weight': 'bal...   \n",
       "3        RandomForest  {'bootstrap': True, 'ccp_alpha': 0.0, 'class_w...   \n",
       "4    GradientBoosting  {'ccp_alpha': 0.0, 'criterion': 'friedman_mse'...   \n",
       "5                 SVC  {'C': 1.0, 'break_ties': False, 'cache_size': ...   \n",
       "6        DecisionTree  {'ccp_alpha': 0.0, 'class_weight': 'balanced',...   \n",
       "7          KNeighbors  {'algorithm': 'auto', 'leaf_size': 30, 'metric...   \n",
       "8          NaiveBayes           {'priors': None, 'var_smoothing': 1e-09}   \n",
       "\n",
       "   Accuracy  F1 Score  Precision  Recall  \n",
       "0      0.40  0.430560   0.489744    0.40  \n",
       "1      0.64  0.604755   0.584286    0.64  \n",
       "2      0.64  0.631019   0.623860    0.64  \n",
       "3      0.64  0.561951   0.500870    0.64  \n",
       "4      0.64  0.604755   0.584286    0.64  \n",
       "5      0.44  0.467222   0.513766    0.44  \n",
       "6      0.56  0.516923   0.480000    0.56  \n",
       "7      0.48  0.506486   0.568462    0.48  \n",
       "8      0.40  0.411429   0.423529    0.40  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "b5fee749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating SARIMAX...\n",
      "SARIMAX MSE: 0.6186, R^2: -2.0683, MAE: 0.5561\n",
      "Evaluating SARIMA_Weekly...\n",
      "SARIMA_Weekly MSE: 0.4803, R^2: -1.3822, MAE: 0.5034\n",
      "Evaluating SARIMA_Monthly...\n",
      "SARIMA_Monthly MSE: 0.3356, R^2: -0.6647, MAE: 0.4062\n",
      "Evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "time_series_evaluation = evaluate_models(time_series_models_trained, X_test, y_test, model_type=\"time_series\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "56fad057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>R^2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SARIMAX</td>\n",
       "      <td>resting_heart_rate                -0.053805\n",
       "av...</td>\n",
       "      <td>0.618576</td>\n",
       "      <td>0.556129</td>\n",
       "      <td>-2.068332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SARIMA_Weekly</td>\n",
       "      <td>intercept                         -0.030818\n",
       "re...</td>\n",
       "      <td>0.480250</td>\n",
       "      <td>0.503433</td>\n",
       "      <td>-1.382194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SARIMA_Monthly</td>\n",
       "      <td>intercept                         -0.097576\n",
       "re...</td>\n",
       "      <td>0.335604</td>\n",
       "      <td>0.406242</td>\n",
       "      <td>-0.664701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model                                         Parameters  \\\n",
       "0         SARIMAX  resting_heart_rate                -0.053805\n",
       "av...   \n",
       "1   SARIMA_Weekly  intercept                         -0.030818\n",
       "re...   \n",
       "2  SARIMA_Monthly  intercept                         -0.097576\n",
       "re...   \n",
       "\n",
       "        MSE       MAE       R^2  \n",
       "0  0.618576  0.556129 -2.068332  \n",
       "1  0.480250  0.503433 -1.382194  \n",
       "2  0.335604  0.406242 -0.664701  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0c1712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080c89e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a183375b-2fe3-4d90-acfd-9de3242af705",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
